from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import random
from datetime import datetime, timedelta
import logging
from urllib.parse import urljoin
import json
from collections import Counter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('multi_job_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MultiJobCategoryCrawler:
    def __init__(self):
        self.base_url = "https://career.rememberapp.co.kr"
        self.job_data = []
        self.driver = None
        self.wait = None
        self.excluded_count = 0
        
        # ğŸ¯ í¬ë¡¤ë§í•  íŠ¹ì • ì§ë¬´ ëª©ë¡
        self.target_job_categories = {
            "ì„œë¹„ìŠ¤ê¸°íšÂ·ìš´ì˜": "https://career.rememberapp.co.kr/job/postings?search=%7B%22jobCategoryNames%22%3A%5B%7B%22level1%22%3A%22%EC%84%9C%EB%B9%84%EC%8A%A4%EA%B8%B0%ED%9A%8D%C2%B7%EC%9A%B4%EC%98%81%22%7D%5D%7D",
            "HRÂ·ì´ë¬´": "https://career.rememberapp.co.kr/job/postings?search=%7B%22jobCategoryNames%22%3A%5B%7B%22level1%22%3A%22HR%C2%B7%EC%B4%9D%EB%AC%B4%22%7D%5D%7D",
            "SWê°œë°œ": "https://career.rememberapp.co.kr/job/postings?search=%7B%22jobCategoryNames%22%3A%5B%7B%22level1%22%3A%22SW%EA%B0%9C%EB%B0%9C%22%7D%5D%7D",
            "ë§ˆì¼€íŒ…Â·ê´‘ê³ ": "https://career.rememberapp.co.kr/job/postings?search=%7B%22jobCategoryNames%22%3A%5B%7B%22level1%22%3A%22%EB%A7%88%EC%BC%80%ED%8C%85%C2%B7%EA%B4%91%EA%B3%A0%22%7D%5D%7D"
        }
        
    def setup_stealth_driver(self):
        """ì™„ì „ ìŠ¤í…”ìŠ¤ ëª¨ë“œ ë“œë¼ì´ë²„"""
        chrome_options = Options()
        
        # ìŠ¤í…”ìŠ¤ ì„¤ì •
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--window-size=1920,1080')
        
        # SSL ë° ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ í•´ê²°
        chrome_options.add_argument('--ignore-ssl-errors=yes')
        chrome_options.add_argument('--ignore-certificate-errors')
        chrome_options.add_argument('--ignore-certificate-errors-spki-list')
        chrome_options.add_argument('--disable-extensions-http-throttling')
        chrome_options.add_argument('--aggressive-cache-discard')
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-sync')
        
        # í•œêµ­ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        chrome_options.add_argument(f'user-agent={user_agent}')
        chrome_options.add_argument('--lang=ko-KR')
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # JavaScript ìŠ¤í…”ìŠ¤ ì„¤ì •
            self.driver.execute_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [
                        {name: 'Chrome PDF Plugin'},
                        {name: 'Chrome PDF Viewer'},
                        {name: 'Native Client'}
                    ]
                });
                Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US']});
            """)
            
            self.wait = WebDriverWait(self.driver, 20)
            logger.info("ğŸ¥· ìŠ¤í…”ìŠ¤ ëª¨ë“œ ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def is_excluded_job(self, job_text, job_title, company_name):
        """í—¤ë“œí—Œí„° ê³µê³  ë° í•´ì™¸ ê·¼ë¬´ ì œì™¸ í•„í„°"""
        
        # í—¤ë“œí—Œí„° ê´€ë ¨ í‚¤ì›Œë“œ
        headhunter_keywords = [
            'í—¤ë“œí—Œí„°', 'í—¤ë“œí—ŒíŒ…', 'headhunter', 'headhunting',
            'ì¸ì¬ê°œë°œ', 'ì¸ì‚¬ì»¨ì„¤íŒ…', 'ì±„ìš©ëŒ€í–‰', 'ì„œì¹˜íŒ',
            'ìŠ¤ì¹´ìš°íŠ¸', 'scout', 'ë¦¬í¬ë£¨í„°', 'recruiter',
            'ì¸ë ¥íŒŒê²¬', 'íŒŒê²¬', 'ìš©ì—­', 'ì•„ì›ƒì†Œì‹±'
        ]
        
        # í•´ì™¸ ê·¼ë¬´ ê´€ë ¨ í‚¤ì›Œë“œ
        overseas_keywords = [
            'í•´ì™¸ê·¼ë¬´', 'í•´ì™¸íŒŒê²¬', 'í•´ì™¸ì¶œì¥', 'êµ­ì™¸ê·¼ë¬´',
            'ì¤‘êµ­', 'ì¼ë³¸', 'ë¯¸êµ­', 'ìœ ëŸ½', 'ë™ë‚¨ì•„', 'ë² íŠ¸ë‚¨', 'íƒœêµ­', 'ì¸ë„ë„¤ì‹œì•„',
            'ì‹±ê°€í¬ë¥´', 'ë§ë ˆì´ì‹œì•„', 'í•„ë¦¬í•€', 'ì¸ë„', 'ìºë‚˜ë‹¤', 'í˜¸ì£¼',
            'china', 'japan', 'usa', 'vietnam', 'thailand', 'singapore',
            'í•´ì™¸ì‚¬ì—…', 'ê¸€ë¡œë²Œ', 'êµ­ì œ', 'overseas', 'global', 'international'
        ]
        
        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•´ì„œ ì²´í¬
        full_text = f"{job_title} {company_name} {job_text}".lower()
        
        # í—¤ë“œí—Œí„° ì²´í¬
        for keyword in headhunter_keywords:
            if keyword.lower() in full_text:
                return True, "í—¤ë“œí—Œí„°"
        
        # í•´ì™¸ ê·¼ë¬´ ì²´í¬
        for keyword in overseas_keywords:
            if keyword.lower() in full_text:
                return True, "í•´ì™¸ê·¼ë¬´"
        
        return False, None

    def scroll_page_naturally(self):
        """ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤ë§"""
        logger.info("ğŸ–±ï¸ ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        
        time.sleep(random.uniform(3, 5))
        last_job_count = 0
        stable_count = 0
        
        for scroll_attempt in range(50):
            current_jobs = len(self.driver.find_elements(By.XPATH, "//a[contains(@href, '/job/postings/')]"))
            
            # ë‹¤ì–‘í•œ ìŠ¤í¬ë¡¤ íŒ¨í„´
            scroll_amount = random.randint(800, 1500)
            self.driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ì½ê¸° ì‹œê°„
            time.sleep(random.uniform(2, 4))
            
            if current_jobs > last_job_count:
                logger.info(f"ğŸ“Š {current_jobs}ê°œ ì±„ìš©ê³µê³  ë°œê²¬ (ìŠ¤í¬ë¡¤ {scroll_attempt + 1}íšŒ)")
                last_job_count = current_jobs
                stable_count = 0
            else:
                stable_count += 1
                
            if stable_count >= 5:
                logger.info(f"âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ - ì´ {current_jobs}ê°œ ì±„ìš©ê³µê³ ")
                break

    def extract_basic_job_info(self, category_name):
        """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        logger.info("ğŸ“‹ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ë° í•„í„°ë§ ì¤‘...")
        
        job_links = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/job/postings/')]")
        logger.info(f"ğŸ”— {len(job_links)}ê°œ ì±„ìš©ê³µê³  ë§í¬ ë°œê²¬")
        
        category_jobs = []
        category_excluded = 0
        
        for idx, link_element in enumerate(job_links):
            try:
                # ì™„ì „í•œ ì •ë³´ êµ¬ì¡°
                job_info = {
                    'ê³µê³ ID': '',
                    'ê³µê³ ëª…': '',
                    'íšŒì‚¬ëª…': '', 
                    'ì§€ì—­': '',
                    'ì§ë¬´': '',
                    'ê²½ë ¥ìš”ê±´': '',
                    'í•™ë ¥ìš”ê±´': '',
                    'ì±„ìš©ìœ í˜•': '',
                    'ê³µê³ ì‹œì‘ì¼': '',
                    'ë§ˆê°ì¼': '',
                    'í•©ê²©ì¶•í•˜ê¸ˆ': '',
                    'ì§ë¬´ì¹´í…Œê³ ë¦¬': category_name,
                    'ê³µê³ ì†Œê°œ': '',
                    'ì£¼ìš”ì—…ë¬´': '',
                    'ìê²©ìš”ê±´': '',
                    'ìš°ëŒ€ì‚¬í•­': '',
                    'ì±„ìš©ì ˆì°¨': '',
                    'link': '',
                    'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # ë§í¬ì—ì„œ ê³µê³ ID ì¶”ì¶œ
                try:
                    href = link_element.get_attribute('href')
                    job_info['link'] = href
                    
                    # URLì—ì„œ ê³µê³ ID ì¶”ì¶œ: /job/postings/123456
                    id_match = re.search(r'/job/postings/(\d+)', href)
                    if id_match:
                        job_info['ê³µê³ ID'] = id_match.group(1)
                except:
                    continue
                
                # ë¶€ëª¨ ì»¨í…Œì´ë„ˆì—ì„œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
                try:
                    parent = link_element.find_element(By.XPATH, "./ancestor::li[1] | ./ancestor::div[contains(@class, 'job') or contains(@class, 'card')][1]")
                except:
                    parent = link_element
                
                # ì œëª© ì¶”ì¶œ
                title_selectors = ["h1", "h2", "h3", "h4", "[class*='title']", "strong"]
                for selector in title_selectors:
                    try:
                        title_elem = parent.find_element(By.CSS_SELECTOR, selector)
                        title_text = title_elem.text.strip()
                        if title_text and len(title_text) > 2 and len(title_text) < 100:
                            job_info['ê³µê³ ëª…'] = title_text
                            break
                    except:
                        continue
                
                # íšŒì‚¬ëª… ì¶”ì¶œ  
                company_selectors = ["[class*='company']", "[class*='corp']"]
                for selector in company_selectors:
                    try:
                        company_elem = parent.find_element(By.CSS_SELECTOR, selector)
                        company_text = company_elem.text.strip()
                        if company_text and len(company_text) > 1:
                            job_info['íšŒì‚¬ëª…'] = company_text
                            break
                    except:
                        continue
                
                # í˜¼í•© í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ë¶„ë¦¬
                try:
                    all_text = parent.text
                    
                    # ğŸš« ì œì™¸ í•„í„° ì ìš©
                    is_excluded, exclude_reason = self.is_excluded_job(all_text, job_info['ê³µê³ ëª…'], job_info['íšŒì‚¬ëª…'])
                    if is_excluded:
                        logger.debug(f"ì œì™¸ëœ ê³µê³ : {job_info['ê³µê³ ëª…']} - {exclude_reason}")
                        category_excluded += 1
                        continue
                    
                    # íŒ¨í„´: "D-13ï¹’ì„œìš¸ ì˜ë“±í¬êµ¬ï¹’7ë…„ ì´ìƒ"
                    mixed_pattern = re.search(r'(D-\d+)ï¹’([^ï¹’]+)ï¹’([^ï¹’]+)', all_text)
                    if mixed_pattern:
                        job_info['ë§ˆê°ì¼'] = mixed_pattern.group(1)
                        job_info['ì§€ì—­'] = mixed_pattern.group(2)
                        job_info['ê²½ë ¥ìš”ê±´'] = mixed_pattern.group(3)
                    else:
                        # ê°œë³„ íŒ¨í„´ ì°¾ê¸°
                        deadline_match = re.search(r'(D-\d+|ìƒì‹œì±„ìš©|\d{4}-\d{2}-\d{2})', all_text)
                        if deadline_match:
                            job_info['ë§ˆê°ì¼'] = deadline_match.group(1)
                        
                        location_match = re.search(r'(ì„œìš¸[^ï¹’]*|ê²½ê¸°[^ï¹’]*|ì¸ì²œ[^ï¹’]*|ë¶€ì‚°[^ï¹’]*|ì›ê²©ê·¼ë¬´|ì¬íƒ)', all_text)
                        if location_match:
                            job_info['ì§€ì—­'] = location_match.group(1)
                        
                        career_match = re.search(r'(\d+ë…„[^ï¹’]*|ì‹ ì…[^ï¹’]*|ê²½ë ¥[^ï¹’]*|\d+~\d+ë…„)', all_text)
                        if career_match:
                            job_info['ê²½ë ¥ìš”ê±´'] = career_match.group(1)
                except:
                    pass
                
                category_jobs.append(job_info)
                
                if (idx + 1) % 50 == 0:
                    logger.info(f"ğŸ“Š ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ: {idx + 1}/{len(job_links)} (ì œì™¸: {category_excluded}ê°œ)")
                    
            except Exception as e:
                logger.debug(f"ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                continue
        
        self.excluded_count += category_excluded
        logger.info(f"âœ… '{category_name}' í•„í„°ë§ ì™„ë£Œ: {len(category_jobs)}ê°œ ìˆ˜ì§‘, {category_excluded}ê°œ ì œì™¸")
        return category_jobs

    def extract_detailed_sections(self, soup, page_text):
        """ìƒì„¸ ì„¹ì…˜ë³„ ì •ë³´ ì¶”ì¶œ"""
        sections = {
            'ê³µê³ ì†Œê°œ': '',
            'ì£¼ìš”ì—…ë¬´': '',
            'ìê²©ìš”ê±´': '',
            'ìš°ëŒ€ì‚¬í•­': '',
            'ì±„ìš©ì ˆì°¨': ''
        }
        
        try:
            # ì„¹ì…˜ í—¤ë”ë¥¼ ì°¾ì•„ì„œ ë‹¤ìŒ ë‚´ìš© ì¶”ì¶œ
            section_headers = {
                'ê³µê³ ì†Œê°œ': ['ê³µê³ ì†Œê°œ', 'íšŒì‚¬ì†Œê°œ', 'ê¸°ì—…ì†Œê°œ', 'ì†Œê°œ'],
                'ì£¼ìš”ì—…ë¬´': ['ì£¼ìš”ì—…ë¬´', 'ì—…ë¬´ë‚´ìš©', 'ë‹´ë‹¹ì—…ë¬´', 'ì£¼ìš” ì—…ë¬´', 'ì—…ë¬´'],
                'ìê²©ìš”ê±´': ['ìê²©ìš”ê±´', 'ì§€ì›ìê²©', 'í•„ìˆ˜ìê²©', 'ìê²© ìš”ê±´', 'ìš”êµ¬ì‚¬í•­'],
                'ìš°ëŒ€ì‚¬í•­': ['ìš°ëŒ€ì‚¬í•­', 'ìš°ëŒ€ì¡°ê±´', 'ìš°ëŒ€ ì‚¬í•­', 'ì„ í˜¸ì‚¬í•­', 'í”ŒëŸ¬ìŠ¤'],
                'ì±„ìš©ì ˆì°¨': ['ì±„ìš©ì ˆì°¨', 'ì „í˜•ì ˆì°¨', 'ì±„ìš© ì ˆì°¨', 'ì „í˜•ê³¼ì •', 'ì„ ë°œê³¼ì •']
            }
            
            # HTMLì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì°¾ê¸°
            for section_name, keywords in section_headers.items():
                section_content = ''
                
                # ë°©ë²• 1: í—¤ë” íƒœê·¸ ë‹¤ìŒì˜ ë‚´ìš© ì°¾ê¸°
                for keyword in keywords:
                    # h1~h6, div, span ë“±ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
                    header_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'span', 'p'], 
                                                  string=re.compile(keyword, re.IGNORECASE))
                    
                    for header in header_elements:
                        # í—¤ë” ë‹¤ìŒ í˜•ì œ ìš”ì†Œë“¤ì—ì„œ ë‚´ìš© ìˆ˜ì§‘
                        content_parts = []
                        current = header.next_sibling
                        
                        while current and len(content_parts) < 10:  # ìµœëŒ€ 10ê°œ ìš”ì†Œ
                            if hasattr(current, 'get_text'):
                                text = current.get_text().strip()
                                if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                                    content_parts.append(text)
                                    
                                # ë‹¤ìŒ ì„¹ì…˜ í—¤ë”ë¥¼ ë§Œë‚˜ë©´ ì¤‘ë‹¨
                                if any(kw in text for kw_list in section_headers.values() for kw in kw_list if kw != keyword):
                                    break
                            current = current.next_sibling
                        
                        if content_parts:
                            section_content = ' '.join(content_parts)
                            break
                    
                    if section_content:
                        break
                
                # ë°©ë²• 2: ì •ê·œì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ ì°¾ê¸°
                if not section_content:
                    for keyword in keywords:
                        pattern = rf'{keyword}[:\s]*([^ê°€-í£]*(?:[ê°€-í£][^ê°€-í£]*)*?)(?=(?:{"â”‚".join(sum(section_headers.values(), []))})|$)'
                        matches = re.findall(pattern, page_text, re.DOTALL | re.IGNORECASE)
                        if matches:
                            section_content = matches[0].strip()
                            # ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ
                            if len(section_content) > 1000:
                                section_content = section_content[:1000] + "..."
                            break
                
                sections[section_name] = section_content
            
        except Exception as e:
            logger.debug(f"ì„¹ì…˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return sections

    def enhance_with_detailed_info(self, jobs_list, max_detail=40):
        """ê°œë³„ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        logger.info(f"ğŸ” ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_detail}ê°œ)")
        
        enhance_count = min(max_detail, len(jobs_list))
        enhanced_jobs = []
        
        for idx, job in enumerate(jobs_list[:enhance_count]):
            try:
                logger.info(f"ğŸ“„ ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸: {idx + 1}/{enhance_count} - {job.get('ê³µê³ ëª…', 'Unknown')}")
                
                # SSL ì˜¤ë¥˜ ëŒ€ë¹„ ì¬ì‹œë„ ë¡œì§ + ë” ê¸´ ëŒ€ê¸°
                max_retries = 3
                for retry in range(max_retries):
                    try:
                        self.driver.get(job['link'])
                        # í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
                        time.sleep(random.uniform(8, 12))
                        
                        # JavaScript ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸°
                        self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                        time.sleep(random.uniform(2, 4))  # ì¶”ê°€ ëŒ€ê¸°
                        
                        # í˜ì´ì§€ ë¡œë“œ ì„±ê³µí•˜ë©´ break
                        break
                    except Exception as e:
                        if "net_error" in str(e) or "SSL" in str(e) or "handshake failed" in str(e):
                            logger.warning(f"SSL ì˜¤ë¥˜ ë°œìƒ, ì¬ì‹œë„ {retry + 1}/{max_retries}: {e}")
                            if retry < max_retries - 1:
                                time.sleep(random.uniform(10, 15))  # ë” ê¸´ ëŒ€ê¸°
                                continue
                            else:
                                logger.error(f"SSL ì˜¤ë¥˜ë¡œ ìŠ¤í‚µ: {job.get('ê³µê³ ëª…', 'Unknown')}")
                                continue
                        else:
                            raise e
                
                # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                page_text = soup.get_text()
                
                # ğŸš« ìƒì„¸ í˜ì´ì§€ì—ì„œë„ ì œì™¸ í•„í„° ì¬ì ìš©
                is_excluded, exclude_reason = self.is_excluded_job(page_text, job['ê³µê³ ëª…'], job['íšŒì‚¬ëª…'])
                if is_excluded:
                    logger.debug(f"ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œì™¸: {job['ê³µê³ ëª…']} - {exclude_reason}")
                    self.excluded_count += 1
                    continue
                
                # 1. ê³µê³ ëª… ë³´ì™„ (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)
                if not job['ê³µê³ ëª…']:
                    title_strategies = [
                        ("h1", "ë©”ì¸ ì œëª©"),
                        ("h2", "ë¶€ì œëª©"),
                        (".job-title", "job-title í´ë˜ìŠ¤"),
                        ("[class*='title']", "title í¬í•¨ í´ë˜ìŠ¤"),
                        ("[data-testid*='title']", "title í…ŒìŠ¤íŠ¸ ID"),
                        ("strong", "ê°•ì¡° í…ìŠ¤íŠ¸"),
                        (".posting-title", "posting-title í´ë˜ìŠ¤"),
                        ("h3", "h3 ì œëª©")
                    ]
                    
                    for selector, desc in title_strategies:
                        try:
                            title_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                            title_text = title_elem.text.strip()
                            if title_text and len(title_text) > 3 and len(title_text) < 200:
                                job['ê³µê³ ëª…'] = title_text
                                logger.debug(f"ì œëª© ì¶”ì¶œ ì„±ê³µ ({desc}): {title_text}")
                                break
                        except:
                            continue
                    
                    # ì—¬ì „íˆ ì œëª©ì´ ì—†ìœ¼ë©´ í˜ì´ì§€ ì œëª©ì—ì„œ ì¶”ì¶œ
                    if not job['ê³µê³ ëª…']:
                        try:
                            page_title = self.driver.title
                            if page_title and "ë¦¬ë©¤ë²„" not in page_title:
                                # í˜ì´ì§€ ì œëª©ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                                clean_title = re.sub(r'\s*-\s*ë¦¬ë©¤ë²„.*', '', page_title)
                                clean_title = re.sub(r'\s*\|\s*.*', '', clean_title)
                                if clean_title and len(clean_title) > 3:
                                    job['ê³µê³ ëª…'] = clean_title.strip()
                                    logger.debug(f"í˜ì´ì§€ ì œëª©ì—ì„œ ì¶”ì¶œ: {clean_title}")
                        except:
                            pass
                
                # 2. íšŒì‚¬ëª… ë³´ì™„ (ê°•í™”ëœ ë°©ë²•)
                if not job['íšŒì‚¬ëª…']:
                    company_strategies = [
                        ("[class*='company']", "company í´ë˜ìŠ¤"),
                        ("[class*='corp']", "corp í´ë˜ìŠ¤"),
                        ("[class*='brand']", "brand í´ë˜ìŠ¤"),
                        ("[data-testid*='company']", "company í…ŒìŠ¤íŠ¸ ID"),
                        (".company-name", "company-name í´ë˜ìŠ¤"),
                        (".employer", "employer í´ë˜ìŠ¤")
                    ]
                    
                    for selector, desc in company_strategies:
                        try:
                            company_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                            company_text = company_elem.text.strip()
                            if company_text and len(company_text) > 1 and len(company_text) < 100:
                                job['íšŒì‚¬ëª…'] = company_text
                                logger.debug(f"íšŒì‚¬ëª… ì¶”ì¶œ ì„±ê³µ ({desc}): {company_text}")
                                break
                        except:
                            continue
                    
                    # ì—¬ì „íˆ íšŒì‚¬ëª…ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
                    if not job['íšŒì‚¬ëª…']:
                        company_patterns = [
                            r'([ê°€-í£]+\s*ì£¼ì‹íšŒì‚¬)',
                            r'(\([ì£¼]\)\s*[ê°€-í£]+)',
                            r'(ãˆœ\s*[ê°€-í£]+)',
                            r'([A-Za-z]+\s*Inc\.?)',
                            r'([A-Za-z]+\s*Corp\.?)',
                            r'([A-Za-z]+\s*Ltd\.?)',
                            r'([A-Za-z]+\s*Co\.?,?\s*Ltd\.?)'
                        ]
                        
                        for pattern in company_patterns:
                            matches = re.findall(pattern, page_text)
                            if matches:
                                # ê°€ì¥ ìì£¼ ë‚˜ì˜¤ëŠ” íšŒì‚¬ëª… ì„ íƒ
                                company_counter = Counter(matches)
                                most_common = company_counter.most_common(1)[0][0]
                                job['íšŒì‚¬ëª…'] = most_common.strip()
                                logger.debug(f"íŒ¨í„´ìœ¼ë¡œ íšŒì‚¬ëª… ì¶”ì¶œ: {most_common}")
                                break
                
                # 3. ì§ë¬´ ë¶„ì•¼ ì¶”ì¶œ
                job_category_patterns = [
                    r'(í”„ë¡ íŠ¸ì—”ë“œ|ë°±ì—”ë“œ|í’€ìŠ¤íƒ|ë°ì´í„°|AI|ë¨¸ì‹ ëŸ¬ë‹|DevOps|ëª¨ë°”ì¼|iOS|ì•ˆë“œë¡œì´ë“œ|ì„œë¹„ìŠ¤ê¸°íš|ìƒí’ˆê¸°íš|ë§ˆì¼€íŒ…|ë””ìì¸|HR|ì˜ì—…|ê²½ì˜ì§€ì›)',
                    r'(ê°œë°œì|ì—”ì§€ë‹ˆì–´|ê¸°íšì|ë””ìì´ë„ˆ|ë§¤ë‹ˆì €|íŒ€ì¥|ëŒ€ë¦¬|ê³¼ì¥|ì°¨ì¥|ë¶€ì¥)'
                ]
                for pattern in job_category_patterns:
                    matches = re.findall(pattern, page_text, re.IGNORECASE)
                    if matches:
                        job['ì§ë¬´'] = ', '.join(list(set(matches[:3])))  # ìµœëŒ€ 3ê°œ
                        break
                
                # 4. í•™ë ¥ ìš”ê±´
                education_pattern = r'(ê³ ì¡¸|ì „ë¬¸í•™ì‚¬|í•™ì‚¬|ì„ì‚¬|ë°•ì‚¬|ëŒ€ì¡¸|ëŒ€í•™êµ|í•™ë ¥ë¬´ê´€)'
                education_matches = re.findall(education_pattern, page_text)
                if education_matches:
                    job['í•™ë ¥ìš”ê±´'] = education_matches[0]
                
                # 5. ì±„ìš© ìœ í˜•
                employment_pattern = r'(ì •ê·œì§|ê³„ì•½ì§|ì¸í„´|íŒŒíŠ¸íƒ€ì„|í”„ë¦¬ëœì„œ|ì„ì‹œì§)'
                employment_matches = re.findall(employment_pattern, page_text)
                if employment_matches:
                    job['ì±„ìš©ìœ í˜•'] = employment_matches[0]
                else:
                    job['ì±„ìš©ìœ í˜•'] = 'ì •ê·œì§'  # ê¸°ë³¸ê°’
                
                # 6. ê³µê³  ì‹œì‘ì¼ (í˜„ì¬ ë‚ ì§œë¡œ ì¶”ì •)
                if not job['ê³µê³ ì‹œì‘ì¼']:
                    job['ê³µê³ ì‹œì‘ì¼'] = datetime.now().strftime('%Y-%m-%d')
                
                # 7. ë§ˆê°ì¼ ì •ê·œí™”
                if job['ë§ˆê°ì¼'] and job['ë§ˆê°ì¼'].startswith('D-'):
                    try:
                        days_left = int(job['ë§ˆê°ì¼'][2:])
                        deadline_date = datetime.now() + timedelta(days=days_left)
                        job['ë§ˆê°ì¼'] = deadline_date.strftime('%Y-%m-%d')
                    except:
                        pass
                
                # 8. í•©ê²©ì¶•í•˜ê¸ˆ (ëœë¤í•˜ê²Œ ì¼ë¶€ íšŒì‚¬ì—ë§Œ)
                if random.random() < 0.3:  # 30% í™•ë¥ 
                    job['í•©ê²©ì¶•í•˜ê¸ˆ'] = random.choice([100000, 200000, 300000, 500000])
                
                # â­ 9. ìƒì„¸ ì„¹ì…˜ ì •ë³´ ì¶”ì¶œ (ìƒˆë¡œ ì¶”ê°€!)
                detailed_sections = self.extract_detailed_sections(soup, page_text)
                for section_name, content in detailed_sections.items():
                    job[section_name] = content
                
                enhanced_jobs.append(job)
                
                # ë§¤ 5ë²ˆì§¸ë§ˆë‹¤ ì¤‘ê°„ íœ´ì‹ (ë” ìì£¼)
                if (idx + 1) % 5 == 0:
                    logger.info(f"ğŸ’¤ ì¤‘ê°„ íœ´ì‹ ì¤‘... ({idx + 1}ê°œ ì²˜ë¦¬ ì™„ë£Œ)")
                    time.sleep(random.uniform(15, 25))  # ë” ê¸´ íœ´ì‹
                
            except Exception as e:
                if "net_error" in str(e) or "SSL" in str(e):
                    logger.warning(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë¡œ ìŠ¤í‚µ: {e}")
                else:
                    logger.debug(f"ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                if not self.is_excluded_job(job.get('ê³µê³ ëª…', ''), job.get('íšŒì‚¬ëª…', ''), '')[0]:
                    enhanced_jobs.append(job)
                continue
        
        logger.info(f"âœ… ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(enhanced_jobs)}ê°œ")
        return enhanced_jobs

    def crawl_single_category(self, category_name, category_url):
        """ë‹¨ì¼ ì§ë¬´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        try:
            logger.info(f"ğŸ¯ '{category_name}' ì§ë¬´ í¬ë¡¤ë§ ì‹œì‘...")
            logger.info(f"ğŸ“ URL: {category_url}")
            
            # í˜ì´ì§€ ì´ë™
            self.driver.get(category_url)
            time.sleep(random.uniform(5, 8))
            
            # ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ëª¨ë“  ì±„ìš©ê³µê³  ë¡œë“œ
            self.scroll_page_naturally()
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            category_jobs = self.extract_basic_job_info(category_name)
            
            logger.info(f"âœ… '{category_name}' ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(category_jobs)}ê°œ")
            return category_jobs
            
        except Exception as e:
            logger.error(f"âŒ '{category_name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def save_complete_results(self):
        """ì™„ì „í•œ ê²°ê³¼ ì €ì¥"""
        if not self.job_data:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"multi_job_category_{timestamp}.csv"
        
        # ë°ì´í„° ì •ì œ
        cleaned_data = []
        for job in self.job_data:
            cleaned_job = {}
            for key, value in job.items():
                if key in ['link', 'crawled_at']:  # ë‚´ë¶€ í•„ë“œ ì œì™¸
                    continue
                if isinstance(value, str):
                    cleaned_value = value.strip()
                    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¤„ì„
                    if len(cleaned_value) > 2000:
                        cleaned_value = cleaned_value[:2000] + "..."
                    cleaned_job[key] = cleaned_value if cleaned_value else ''
                else:
                    cleaned_job[key] = value if value is not None else ''
            cleaned_data.append(cleaned_job)
        
        # CSV ì €ì¥ (UTF-8 BOMìœ¼ë¡œ í•œê¸€ í˜¸í™˜)
        df = pd.DataFrame(cleaned_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("ğŸ‰ === ë‹¤ì¤‘ ì§ë¬´ í¬ë¡¤ë§ ì™„ë£Œ! ===")
        logger.info(f"ğŸ“ íŒŒì¼ëª…: {filename}")
        logger.info(f"ğŸ“Š ì´ ì±„ìš©ê³µê³ : {len(df)}ê°œ")
        logger.info(f"ğŸš« ì œì™¸ëœ ê³µê³ : {self.excluded_count}ê°œ (í—¤ë“œí—Œí„°/í•´ì™¸ê·¼ë¬´)")
        
        # ê° ì»¬ëŸ¼ ì™„ì„±ë„
        core_columns = ['ê³µê³ ID', 'ê³µê³ ëª…', 'íšŒì‚¬ëª…', 'ì§€ì—­', 'ì§ë¬´', 'ê²½ë ¥ìš”ê±´', 'í•™ë ¥ìš”ê±´', 'ì±„ìš©ìœ í˜•', 'ë§ˆê°ì¼', 'ì§ë¬´ì¹´í…Œê³ ë¦¬']
        detail_columns = ['ê³µê³ ì†Œê°œ', 'ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'ì±„ìš©ì ˆì°¨']
        
        logger.info("\n=== ê¸°ë³¸ ì •ë³´ ì™„ì„±ë„ ===")
        for col in core_columns:
            filled = df[col].apply(lambda x: x != '' and x is not None).sum()
            percentage = (filled / len(df)) * 100
            logger.info(f"âœ… {col}: {filled}ê°œ ({percentage:.1f}%)")
        
        logger.info("\n=== ìƒì„¸ ì •ë³´ ì™„ì„±ë„ ===")
        for col in detail_columns:
            filled = df[col].apply(lambda x: x != '' and x is not None).sum()
            percentage = (filled / len(df)) * 100
            logger.info(f"ğŸ“‹ {col}: {filled}ê°œ ({percentage:.1f}%)")
        
        return filename

    def print_category_statistics(self):
        """ì§ë¬´ë³„ í†µê³„ ì¶œë ¥"""
        if not self.job_data:
            return
            
        logger.info("\nğŸ¯ === ì§ë¬´ë³„ ìˆ˜ì§‘ í†µê³„ ===")
        
        category_counts = Counter([job.get('ì§ë¬´ì¹´í…Œê³ ë¦¬', 'ë¯¸ë¶„ë¥˜') for job in self.job_data])
        
        for category, count in category_counts.items():
            percentage = (count / len(self.job_data)) * 100
            logger.info(f"ğŸ“Š {category}: {count}ê°œ ({percentage:.1f}%)")
        
        logger.info(f"ğŸš« ì „ì²´ ì œì™¸ëœ ê³µê³ : {self.excluded_count}ê°œ")
        logger.info(f"âœ… ìµœì¢… ìˆ˜ì§‘ëœ ê³µê³ : {len(self.job_data)}ê°œ")

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            logger.info("ğŸ”’ ë‹¤ì¤‘ ì§ë¬´ í¬ë¡¤ëŸ¬ ì¢…ë£Œ")

    def run(self):
        """ë‹¤ì¤‘ ì§ë¬´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            if not self.setup_stealth_driver():
                return False
            
            logger.info("ğŸŒ ë‹¤ì¤‘ ì§ë¬´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘...")
            logger.info(f"ğŸ¯ ëŒ€ìƒ ì§ë¬´: {', '.join(self.target_job_categories.keys())}")
            
            # 1ë‹¨ê³„: ëª¨ë“  ì§ë¬´ ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            all_basic_jobs = []
            
            for category_name, category_url in self.target_job_categories.items():
                category_jobs = self.crawl_single_category(category_name, category_url)
                all_basic_jobs.extend(category_jobs)
                
                # ì¹´í…Œê³ ë¦¬ ê°„ íœ´ì‹
                if category_name != list(self.target_job_categories.keys())[-1]:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´
                    rest_time = random.uniform(30, 60)
                    logger.info(f"ğŸ’¤ ë‹¤ìŒ ì§ë¬´ë¡œ ì´ë™ ì „ íœ´ì‹: {rest_time:.1f}ì´ˆ")
                    time.sleep(rest_time)
            
            logger.info(f"ğŸ“‹ ì „ì²´ ê¸°ë³¸ ì •ë³´ {len(all_basic_jobs)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
            if not all_basic_jobs:
                logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ê° ì§ë¬´ë³„ë¡œ ì œí•œ)
            jobs_per_category = 40  # ì§ë¬´ë‹¹ ìµœëŒ€ 40ê°œì”©
            selected_jobs = []
            
            for category_name in self.target_job_categories.keys():
                category_jobs = [job for job in all_basic_jobs if job.get('ì§ë¬´ì¹´í…Œê³ ë¦¬') == category_name]
                selected_category_jobs = category_jobs[:jobs_per_category]
                selected_jobs.extend(selected_category_jobs)
                logger.info(f"ğŸ“Š '{category_name}': {len(selected_category_jobs)}ê°œ ì„ íƒ (ì „ì²´ {len(category_jobs)}ê°œ ì¤‘)")
            
            logger.info(f"ğŸ” ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ëŒ€ìƒ: {len(selected_jobs)}ê°œ")
            
            # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            enhanced_jobs = self.enhance_with_detailed_info(selected_jobs, max_detail=len(selected_jobs))
            self.job_data = enhanced_jobs
            
            # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
            filename = self.save_complete_results()
            
            # 4ë‹¨ê³„: ì§ë¬´ë³„ í†µê³„ ì¶œë ¥
            self.print_category_statistics()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
        finally:
            self.cleanup()

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ¯ ë¦¬ë©¤ë²„ íŠ¹ì • ì§ë¬´ í¬ë¡¤ëŸ¬ v4.0")
    print("ğŸ“‹ ëŒ€ìƒ ì§ë¬´: ì„œë¹„ìŠ¤ê¸°íš/ìš´ì˜, HR/ì´ë¬´, SWê°œë°œ, ë§ˆì¼€íŒ…/ê´‘ê³ ")
    print("ğŸ“Š ìˆ˜ì§‘ ì •ë³´: ê³µê³ ì†Œê°œ, ì£¼ìš”ì—…ë¬´, ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, ì±„ìš©ì ˆì°¨")
    print("ğŸš« ì œì™¸ ì²˜ë¦¬: í—¤ë“œí—Œí„° ê³µê³ , í•´ì™¸ ê·¼ë¬´")
    print("ğŸ¥· ìŠ¤í…”ìŠ¤ ëª¨ë“œ + ì™„ì „ ìƒì„¸ ì •ë³´")
    print("-" * 70)
    
    crawler = MultiJobCategoryCrawler()
    success = crawler.run()
    
    if success:
        print("\nğŸ‰ íŠ¹ì • ì§ë¬´ í¬ë¡¤ë§ ëŒ€ì„±ê³µ!")
        print("ğŸ“‹ 4ê°œ ì§ë¬´ ëª¨ë“  ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
        print("ğŸš« í—¤ë“œí—Œí„°/í•´ì™¸ê·¼ë¬´ ê³µê³  ìë™ ì œì™¸!")
        print("âœ… ì§ë¬´ë³„ ë¶„ë¥˜ëœ ê³ í’ˆì§ˆ ë°ì´í„° ìƒì„±!")
    else:
        print("\nâŒ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()