import requests
import pandas as pd
import time
import random
import logging
import sqlite3
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import os
from pathlib import Path
import urllib.parse
from dataclasses import dataclass, asdict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('jumpit_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class JobPosting:
    """ì±„ìš©ê³µê³  ë°ì´í„° í´ë˜ìŠ¤"""
    position_id: str = ""
    title: str = ""
    company_name: str = ""
    company_id: str = ""
    location: str = ""
    career_level: str = ""
    employment_type: str = ""
    salary: str = ""
    tech_stacks: str = ""
    benefits: str = ""
    job_category: str = ""
    description: str = ""
    requirements: str = ""
    preferred_qualifications: str = ""
    company_description: str = ""
    company_size: str = ""
    company_industry: str = ""
    posted_date: str = ""
    deadline: str = ""
    application_count: int = 0
    view_count: int = 0
    bookmark_count: int = 0
    response_rate: float = 0.0
    tags: str = ""
    work_location_type: str = ""
    experience_years: str = ""
    education_level: str = ""
    crawled_at: str = ""
    api_url: str = ""

class JumpitCrawler:
    def __init__(self, db_name: str = "jumpit_jobs.db"):
        self.base_api_url = "https://jumpit-api.saramin.co.kr/api/positions"
        self.session = requests.Session()
        self.db_name = db_name
        self.job_data: List[JobPosting] = []
        
        # ìš”ì²­ í—¤ë” ì„¤ì • (í•œêµ­ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.jumpit.co.kr/',
            'Origin': 'https://www.jumpit.co.kr',
            'DNT': '1',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'cross-site'
        }
        self.session.headers.update(self.headers)
        
        # ì í• ì‹¤ì œ ì§ë¬´ ë¶„ë¥˜ì— ë§ì¶˜ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        self.search_params = {
            # ğŸ¯ í•µì‹¬ ê°œë°œ ì§ë¬´ (ì í• ì‹¤ì œ ì¹´í…Œê³ ë¦¬ëª… ì‚¬ìš©)
            "ì„œë²„/ë°±ì—”ë“œê°œë°œì": {"jobCategory": "ì„œë²„/ë°±ì—”ë“œ ê°œë°œì", "sort": "rsp_rate"},
            "í”„ë¡ íŠ¸ì—”ë“œê°œë°œì": {"jobCategory": "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì", "sort": "rsp_rate"},
            "ì›¹í’€ìŠ¤íƒê°œë°œì": {"jobCategory": "ì›¹ í’€ìŠ¤íƒ ê°œë°œì", "sort": "rsp_rate"},
            "ì•ˆë“œë¡œì´ë“œê°œë°œì": {"jobCategory": "ì•ˆë“œë¡œì´ë“œ ê°œë°œì", "sort": "rsp_rate"},
            "iOSê°œë°œì": {"jobCategory": "iOS ê°œë°œì", "sort": "rsp_rate"},
            
            # ğŸ”§ ì‹œìŠ¤í…œ/ì¸í”„ë¼
            "í¬ë¡œìŠ¤í”Œë«í¼ì•±ê°œë°œì": {"jobCategory": "í¬ë¡œìŠ¤í”Œë«í¼ ì•±ê°œë°œì", "sort": "rsp_rate"},
            "ê²Œì„í´ë¼ì´ì–¸íŠ¸ê°œë°œì": {"jobCategory": "ê²Œì„ í´ë¼ì´ì–¸íŠ¸ ê°œë°œì", "sort": "rsp_rate"},
            "ê²Œì„ì„œë²„ê°œë°œì": {"jobCategory": "ê²Œì„ ì„œë²„ ê°œë°œì", "sort": "rsp_rate"},
            "DBA": {"jobCategory": "DBA", "sort": "rsp_rate"},
            "DevOpsì‹œìŠ¤í…œì—”ì§€ë‹ˆì–´": {"jobCategory": "devops/ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´", "sort": "rsp_rate"},
            
            # ğŸ“Š ë°ì´í„°/AI
            "ë¹…ë°ì´í„°ì—”ì§€ë‹ˆì–´": {"jobCategory": "ë¹…ë°ì´í„° ì—”ì§€ë‹ˆì–´", "sort": "rsp_rate"},
            "ì¸ê³µì§€ëŠ¥ë¨¸ì‹ ëŸ¬ë‹": {"jobCategory": "ì¸ê³µì§€ëŠ¥/ë¨¸ì‹ ëŸ¬ë‹", "sort": "rsp_rate"},
            
            # ğŸ¨ ê¸°íš/ë””ìì¸/ê´€ë¦¬
            "ì •ë³´ë³´ì•ˆë‹´ë‹¹ì": {"jobCategory": "ì •ë³´ë³´ì•ˆ ë‹´ë‹¹ì", "sort": "rsp_rate"},
            "QAì—”ì§€ë‹ˆì–´": {"jobCategory": "QA ì—”ì§€ë‹ˆì–´", "sort": "rsp_rate"},
            "ê°œë°œPM": {"jobCategory": "ê°œë°œ PM", "sort": "rsp_rate"},
            "HWì„ë² ë””ë“œ": {"jobCategory": "HW/ì„ë² ë””ë“œ", "sort": "rsp_rate"},
            "SWì†”ë£¨ì…˜": {"jobCategory": "SW/ì†”ë£¨ì…˜", "sort": "rsp_rate"},
            "ì›¹í¼ë¸”ë¦¬ì…”": {"jobCategory": "ì›¹í¼ë¸”ë¦¬ì…”", "sort": "rsp_rate"},
            "VR_AR_3D": {"jobCategory": "VR/AR/3D", "sort": "rsp_rate"},
            "ë¸”ë¡ì²´ì¸": {"jobCategory": "ë¸”ë¡ì²´ì¸", "sort": "rsp_rate"},
            "ê¸°ìˆ ì§€ì›": {"jobCategory": "ê¸°ìˆ ì§€ì›", "sort": "rsp_rate"},
            
            # ğŸ” íŠ¹ë³„ ê²€ìƒ‰ ì¡°ê±´ (ì‹¤ì œ ì í• íŒŒë¼ë¯¸í„°)
            "ì¬íƒê·¼ë¬´ê°€ëŠ¥": {"workFromHome": "true", "sort": "reg_dt"},
            "ì‹ ì…í™˜ì˜": {"newcomer": "true", "sort": "reg_dt"},
            "ê³ ì—°ë´‰_5ì²œì´ìƒ": {"minSalary": "5000", "sort": "salary"},
            "ê³ ì—°ë´‰_1ì–µì´ìƒ": {"minSalary": "10000", "sort": "salary"},
            "ê²½ë ¥3ë…„ì´ìƒ": {"minCareer": "3", "sort": "rsp_rate"},
            "ê²½ë ¥5ë…„ì´ìƒ": {"minCareer": "5", "sort": "rsp_rate"},
            "ìƒì‹œì±„ìš©": {"alwaysOpen": "true", "sort": "reg_dt"},
            
            # ğŸ’» ì£¼ìš” ê¸°ìˆ ìŠ¤íƒë³„ ê²€ìƒ‰
            "JavaìŠ¤íƒ": {"techStack": "Java", "sort": "rsp_rate"},
            "PythonìŠ¤íƒ": {"techStack": "Python", "sort": "rsp_rate"},
            "JavaScriptìŠ¤íƒ": {"techStack": "JavaScript", "sort": "rsp_rate"},
            "ReactìŠ¤íƒ": {"techStack": "React", "sort": "rsp_rate"},
            "VueìŠ¤íƒ": {"techStack": "Vue.js", "sort": "rsp_rate"},
            "SpringBootìŠ¤íƒ": {"techStack": "Spring Boot", "sort": "rsp_rate"},
            "NodeJSìŠ¤íƒ": {"techStack": "Node.js", "sort": "rsp_rate"},
            "DockerìŠ¤íƒ": {"techStack": "Docker", "sort": "rsp_rate"},
            "KubernetesìŠ¤íƒ": {"techStack": "Kubernetes", "sort": "rsp_rate"},
            "AWSìŠ¤íƒ": {"techStack": "AWS", "sort": "rsp_rate"},
            "MongoDBìŠ¤íƒ": {"techStack": "MongoDB", "sort": "rsp_rate"},
            "PostgreSQLìŠ¤íƒ": {"techStack": "PostgreSQL", "sort": "rsp_rate"},
            
            # ğŸ¢ ì§€ì—­ë³„ ê²€ìƒ‰
            "ì„œìš¸_ê°•ë‚¨": {"location": "ì„œìš¸ ê°•ë‚¨êµ¬", "sort": "reg_dt"},
            "ì„œìš¸_ì„±ìˆ˜": {"location": "ì„œìš¸ ì„±ë™êµ¬", "sort": "reg_dt"},
            "íŒêµ_ë¶„ë‹¹": {"location": "ê²½ê¸° ì„±ë‚¨ì‹œ", "sort": "reg_dt"},
            "ë¶€ì‚°ì§€ì—­": {"location": "ë¶€ì‚°", "sort": "reg_dt"}
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.init_database()
        
    def init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_name)
            cursor = conn.cursor()
            
            # ì±„ìš©ê³µê³  í…Œì´ë¸” ìƒì„±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS job_postings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    position_id TEXT UNIQUE,
                    title TEXT,
                    company_name TEXT,
                    company_id TEXT,
                    location TEXT,
                    career_level TEXT,
                    employment_type TEXT,
                    salary TEXT,
                    tech_stacks TEXT,
                    benefits TEXT,
                    job_category TEXT,
                    description TEXT,
                    requirements TEXT,
                    preferred_qualifications TEXT,
                    company_description TEXT,
                    company_size TEXT,
                    company_industry TEXT,
                    posted_date TEXT,
                    deadline TEXT,
                    application_count INTEGER,
                    view_count INTEGER,
                    bookmark_count INTEGER,
                    response_rate REAL,
                    tags TEXT,
                    work_location_type TEXT,
                    experience_years TEXT,
                    education_level TEXT,
                    crawled_at TEXT,
                    api_url TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # í¬ë¡¤ë§ ë¡œê·¸ í…Œì´ë¸” ìƒì„±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS crawling_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    search_type TEXT,
                    total_found INTEGER,
                    successfully_crawled INTEGER,
                    failed_requests INTEGER,
                    start_time TEXT,
                    end_time TEXT,
                    duration_seconds REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ '{self.db_name}' ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def make_safe_request(self, url: str, params: Dict = None, max_retries: int = 3) -> Optional[Dict]:
        """ì•ˆì „í•œ API ìš”ì²­ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(max_retries):
            try:
                # ìš”ì²­ ì „ ëœë¤ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                delay = random.uniform(2, 5) + (attempt * 2)  # ì¬ì‹œë„ ì‹œ ë” ê¸´ ëŒ€ê¸°
                time.sleep(delay)
                
                logger.info(f"ğŸ”„ API ìš”ì²­ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                logger.debug(f"URL: {url}")
                logger.debug(f"Params: {params}")
                
                response = self.session.get(url, params=params, timeout=30)
                
                # ìƒíƒœ ì½”ë“œ í™•ì¸
                if response.status_code == 200:
                    data = response.json()
                    logger.info(f"âœ… API ìš”ì²­ ì„±ê³µ (ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes)")
                    return data
                elif response.status_code == 429:  # Too Many Requests
                    wait_time = random.uniform(30, 60)
                    logger.warning(f"âš ï¸ Rate limit ë„ë‹¬, {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.warning(f"âš ï¸ HTTP {response.status_code}: {response.text[:200]}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"â° ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1})")
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"ğŸ”Œ ì—°ê²° ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
            except requests.exceptions.RequestException as e:
                logger.warning(f"ğŸ“¡ ìš”ì²­ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
            except json.JSONDecodeError as e:
                logger.warning(f"ğŸ“„ JSON íŒŒì‹± ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
            
            if attempt < max_retries - 1:
                wait_time = random.uniform(10, 20) * (attempt + 1)
                logger.info(f"ğŸ’¤ {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
        
        logger.error("âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
        return None
    
    def parse_job_posting(self, job_data: Dict, search_type: str = "") -> JobPosting:
        """ì‹¤ì œ ì í• API ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶˜ JobPosting ê°ì²´ ë³€í™˜"""
        try:
            # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
            def safe_get(data, key, default=""):
                try:
                    result = data.get(key, default)
                    return str(result) if result is not None else default
                except (TypeError, AttributeError):
                    return default
            
            def safe_get_int(data, key, default=0):
                try:
                    result = data.get(key, default)
                    return int(result) if result is not None else default
                except (TypeError, ValueError, AttributeError):
                    return default
            
            def safe_get_float(data, key, default=0.0):
                try:
                    result = data.get(key, default)
                    return float(result) if result is not None else default
                except (TypeError, ValueError, AttributeError):
                    return default
            
            def safe_get_list(data, key, default=""):
                try:
                    result = data.get(key, [])
                    if isinstance(result, list):
                        return ", ".join([str(item) for item in result if item])
                    return str(result) if result is not None else default
                except (TypeError, AttributeError):
                    return default
            
            # ğŸ¯ ì í• ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶˜ íŒŒì‹±
            
            # ê¸°ë³¸ ì •ë³´
            position_id = safe_get(job_data, "id")
            title = safe_get(job_data, "title")
            company_name = safe_get(job_data, "companyName")
            job_category = safe_get(job_data, "jobCategory")
            
            # ìœ„ì¹˜ ì •ë³´ (ë°°ì—´ í˜•íƒœ)
            locations = job_data.get("locations", [])
            location = ", ".join(locations) if isinstance(locations, list) else str(locations)
            
            # ê¸°ìˆ  ìŠ¤íƒ (ë°°ì—´ í˜•íƒœ)
            tech_stacks = job_data.get("techStacks", [])
            tech_stacks_str = ", ".join(tech_stacks) if isinstance(tech_stacks, list) else str(tech_stacks)
            
            # ê²½ë ¥ ì •ë³´
            min_career = safe_get_int(job_data, "minCareer")
            max_career = safe_get_int(job_data, "maxCareer")
            is_newcomer = job_data.get("newcomer", False)
            
            # ê²½ë ¥ ìš”ê±´ ë¬¸ìì—´ ìƒì„±
            if is_newcomer:
                career_level = "ì‹ ì… í™˜ì˜"
            elif min_career == 0 and max_career == 0:
                career_level = "ê²½ë ¥ë¬´ê´€"
            elif min_career == max_career:
                career_level = f"{min_career}ë…„"
            else:
                career_level = f"{min_career}~{max_career}ë…„"
            
            # í†µê³„ ì •ë³´
            view_count = safe_get_int(job_data, "viewCount")
            scrap_count = safe_get_int(job_data, "scrapCount")
            celebration = safe_get_int(job_data, "celebration")  # í•©ê²©ì¶•í•˜ê¸ˆ
            
            # ë‚ ì§œ ì •ë³´
            closed_at = safe_get(job_data, "closedAt")
            always_open = job_data.get("alwaysOpen", False)
            
            # ë§ˆê°ì¼ ì²˜ë¦¬
            deadline = ""
            if always_open:
                deadline = "ìƒì‹œì±„ìš©"
            elif closed_at:
                try:
                    # ISO í˜•ì‹ ë‚ ì§œë¥¼ ì¼ë°˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    from datetime import datetime
                    dt = datetime.fromisoformat(closed_at.replace('T', ' ').replace('Z', ''))
                    deadline = dt.strftime('%Y-%m-%d')
                except:
                    deadline = closed_at
            
            # ì´ë¯¸ì§€ ë° ë¡œê³ 
            image_path = safe_get(job_data, "imagePath")
            logo_path = safe_get(job_data, "logo")
            
            # ìˆ¨ê²¨ì§„ ê³µê³  ì—¬ë¶€
            hidden_position = job_data.get("hiddenPosition", False)
            
            # ì§€ì› ì—¬ë¶€
            applied = job_data.get("applied", False)
            scraped = job_data.get("scraped", False)
            
            posting = JobPosting(
                position_id=str(position_id),
                title=title,
                company_name=company_name,
                company_id=safe_get(job_data, "serialNumber"),
                location=location,
                career_level=career_level,
                employment_type="ì •ê·œì§",  # ê¸°ë³¸ê°’ (APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ)
                salary=f"{celebration}ë§Œì›" if celebration > 0 else "",
                tech_stacks=tech_stacks_str,
                benefits="",  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                job_category=job_category,
                description="",  # ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                requirements="",  # ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                preferred_qualifications="",  # ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                company_description="",  # ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                company_size="",  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                company_industry="",  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                posted_date="",  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                deadline=deadline,
                application_count=0,  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                view_count=view_count,
                bookmark_count=scrap_count,
                response_rate=0.0,  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                tags="ì‹ ì…í™˜ì˜" if is_newcomer else "",
                work_location_type="",  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                experience_years=f"{min_career}-{max_career}ë…„",
                education_level="",  # APIì—ì„œ ì œê³µë˜ì§€ ì•ŠìŒ
                crawled_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                api_url=search_type
            )
            
            return posting
            
        except Exception as e:
            logger.error(f"âŒ ì±„ìš©ê³µê³  íŒŒì‹± ì˜¤ë¥˜: {e}")
            logger.debug(f"ë¬¸ì œ ë°ì´í„°: {job_data}")
            return JobPosting()
    
    def crawl_search_type(self, search_name: str, params: Dict, max_pages: int = 10) -> List[JobPosting]:
        """íŠ¹ì • ê²€ìƒ‰ ì¡°ê±´ìœ¼ë¡œ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
        logger.info(f"ğŸ¯ '{search_name}' ê²€ìƒ‰ ì‹œì‘...")
        logger.info(f"ğŸ“‹ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: {params}")
        
        search_jobs = []
        page = 1
        total_found = 0
        failed_requests = 0
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        base_params = {
            "highlight": "false",
            "page": page,
            "limit": 20  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê³µê³  ìˆ˜
        }
        base_params.update(params)
        
        while page <= max_pages:
            logger.info(f"ğŸ“„ í˜ì´ì§€ {page}/{max_pages} ì²˜ë¦¬ ì¤‘...")
            
            base_params["page"] = page
            
            # API ìš”ì²­
            response_data = self.make_safe_request(self.base_api_url, base_params)
            
            if not response_data:
                failed_requests += 1
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨")
                if failed_requests >= 3:  # ì—°ì† 3ë²ˆ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                    logger.error("âŒ ì—°ì† ì‹¤íŒ¨ë¡œ ê²€ìƒ‰ ì¤‘ë‹¨")
                    break
                page += 1
                continue
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì¡° í™•ì¸ - ì í• ì‹¤ì œ API êµ¬ì¡°
            jobs_list = []
            total_count = 0
            current_page = page
            
            # ì í• API ì‘ë‹µ êµ¬ì¡°: {result: {totalCount, page, positions: [...]}}
            if "result" in response_data:
                result_data = response_data["result"]
                
                # ì´ ê°œìˆ˜ í™•ì¸
                total_count = result_data.get("totalCount", 0)
                current_page = result_data.get("page", page)
                
                # positions ë°°ì—´ì—ì„œ ì±„ìš©ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                if "positions" in result_data:
                    jobs_list = result_data["positions"]
                
                logger.info(f"ğŸ“Š API ì‘ë‹µ: ì´ {total_count}ê°œ ì¤‘ í˜ì´ì§€ {current_page}")
                
            # ë°±ì—…: ë‹¤ë¥¸ ì‘ë‹µ êµ¬ì¡°ë„ ì²˜ë¦¬
            elif "positions" in response_data:
                jobs_list = response_data["positions"]
                total_count = response_data.get("totalCount", len(jobs_list))
            elif "data" in response_data:
                if isinstance(response_data["data"], list):
                    jobs_list = response_data["data"]
                elif "positions" in response_data["data"]:
                    jobs_list = response_data["data"]["positions"]
            elif isinstance(response_data, list):
                jobs_list = response_data
            
            if not jobs_list:
                logger.info(f"ğŸ“­ í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                break
            
            logger.info(f"ğŸ“Š í˜ì´ì§€ {page}ì—ì„œ {len(jobs_list)}ê°œ ê³µê³  ë°œê²¬ (ì „ì²´ {total_count}ê°œ)")
            total_found += len(jobs_list)
            
            # ê° ì±„ìš©ê³µê³  íŒŒì‹±
            page_jobs = []
            for job_data in jobs_list:
                try:
                    # ìˆ¨ê²¨ì§„ ê³µê³ ëŠ” ì œì™¸
                    if job_data.get("hiddenPosition", False):
                        continue
                        
                    job_posting = self.parse_job_posting(job_data, search_name)
                    if job_posting.position_id:  # ìœ íš¨í•œ ë°ì´í„°ì¸ ê²½ìš°
                        page_jobs.append(job_posting)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ê³µê³  íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            search_jobs.extend(page_jobs)
            logger.info(f"âœ… í˜ì´ì§€ {page}: {len(page_jobs)}ê°œ íŒŒì‹± ì™„ë£Œ")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ í™•ì¸ - ì í• ìŠ¤íƒ€ì¼
            has_more = False
            
            # 1. totalCount ê¸°ë°˜ íŒë‹¨
            if total_count > 0:
                items_per_page = base_params.get("limit", 20)
                max_possible_pages = (total_count + items_per_page - 1) // items_per_page
                has_more = page < max_possible_pages and page < max_pages
                
            # 2. í˜„ì¬ í˜ì´ì§€ ê²°ê³¼ ê¸°ë°˜ íŒë‹¨
            elif len(jobs_list) == base_params.get("limit", 20):
                has_more = page < max_pages
                
            # 3. ë¹ˆ ê²°ê³¼ í™•ì¸
            elif len(jobs_list) == 0:
                has_more = False
            
            if not has_more:
                logger.info(f"âœ… í˜ì´ì§€ë„¤ì´ì…˜ ì™„ë£Œ (ì´ í˜ì´ì§€: {page})")
                break
            
            page += 1
            
            # í˜ì´ì§€ ê°„ ì ì ˆí•œ íœ´ì‹
            if page <= max_pages:
                rest_time = random.uniform(3, 8)
                logger.info(f"ğŸ’¤ ë‹¤ìŒ í˜ì´ì§€ ë¡œë”© ì „ {rest_time:.1f}ì´ˆ íœ´ì‹...")
                time.sleep(rest_time)
        
        logger.info(f"âœ… '{search_name}' ê²€ìƒ‰ ì™„ë£Œ: {len(search_jobs)}ê°œ ìˆ˜ì§‘ (ì´ {total_found}ê°œ ë°œê²¬)")
        return search_jobs
    
    def save_to_database(self, jobs: List[JobPosting]) -> int:
        """ì±„ìš©ê³µê³ ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¤‘ë³µ ì²˜ë¦¬)"""
        if not jobs:
            return 0
        
        try:
            conn = sqlite3.connect(self.db_name)
            cursor = conn.cursor()
            
            saved_count = 0
            updated_count = 0
            
            for job in jobs:
                # ê¸°ì¡´ ë°ì´í„° í™•ì¸
                cursor.execute("SELECT id FROM job_postings WHERE position_id = ?", (job.position_id,))
                existing = cursor.fetchone()
                
                job_dict = asdict(job)
                
                if existing:
                    # ì—…ë°ì´íŠ¸
                    update_fields = ", ".join([f"{k} = ?" for k in job_dict.keys()])
                    cursor.execute(
                        f"UPDATE job_postings SET {update_fields} WHERE position_id = ?",
                        list(job_dict.values()) + [job.position_id]
                    )
                    updated_count += 1
                else:
                    # ìƒˆë¡œ ì‚½ì…
                    placeholders = ", ".join(["?" for _ in job_dict])
                    fields = ", ".join(job_dict.keys())
                    cursor.execute(
                        f"INSERT INTO job_postings ({fields}) VALUES ({placeholders})",
                        list(job_dict.values())
                    )
                    saved_count += 1
            
            conn.commit()
            conn.close()
            
            logger.info(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {saved_count}ê°œ, ì—…ë°ì´íŠ¸ {updated_count}ê°œ")
            return saved_count + updated_count
            
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0
    
    def save_crawling_log(self, search_type: str, total_found: int, successfully_crawled: int, 
                         failed_requests: int, start_time: str, end_time: str, duration: float):
        """í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_name)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO crawling_logs 
                (search_type, total_found, successfully_crawled, failed_requests, 
                 start_time, end_time, duration_seconds)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (search_type, total_found, successfully_crawled, failed_requests,
                  start_time, end_time, duration))
            
            conn.commit()
            conn.close()
            logger.info("ğŸ“ í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def export_to_csv(self, filename: str = None) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"jumpit_jobs_{timestamp}.csv"
        
        try:
            conn = sqlite3.connect(self.db_name)
            
            # ì±„ìš©ê³µê³  ë°ì´í„° ì¡°íšŒ
            query = """
                SELECT position_id, title, company_name, location, career_level, 
                       employment_type, salary, tech_stacks, benefits, job_category,
                       description, requirements, preferred_qualifications,
                       company_description, company_size, company_industry,
                       posted_date, deadline, application_count, view_count,
                       bookmark_count, response_rate, tags, work_location_type,
                       experience_years, education_level, crawled_at, api_url
                FROM job_postings 
                ORDER BY crawled_at DESC
            """
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            # CSV ì €ì¥ (í•œê¸€ í˜¸í™˜)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"ğŸ“Š CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename} ({len(df)}ê°œ ë ˆì½”ë“œ)")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return ""
    
    def get_database_stats(self) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_name)
            cursor = conn.cursor()
            
            # ì „ì²´ ê³µê³  ìˆ˜
            cursor.execute("SELECT COUNT(*) FROM job_postings")
            total_jobs = cursor.fetchone()[0]
            
            # íšŒì‚¬ë³„ ê³µê³  ìˆ˜ (ìƒìœ„ 10ê°œ)
            cursor.execute("""
                SELECT company_name, COUNT(*) as job_count 
                FROM job_postings 
                WHERE company_name != ''
                GROUP BY company_name 
                ORDER BY job_count DESC 
                LIMIT 10
            """)
            top_companies = cursor.fetchall()
            
            # ğŸ”¥ ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒë³„ í†µê³„ (ì í• ìŠ¤íƒ€ì¼)
            tech_stats = {}
            major_techs = [
                'Java', 'Python', 'JavaScript', 'React', 'Vue.js', 'Node.js',
                'Spring Boot', 'Django', 'PHP', 'C++', 'C#', 'AWS',
                'MySQL', 'Oracle', 'Docker', 'Kubernetes'
            ]
            
            for tech in major_techs:
                cursor.execute("""
                    SELECT COUNT(*) as count
                    FROM job_postings 
                    WHERE tech_stacks LIKE ?
                """, (f'%{tech}%',))
                count = cursor.fetchone()[0]
                if count > 0:
                    tech_stats[tech] = count
            
            # ì§ë¬´ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            cursor.execute("""
                SELECT api_url, COUNT(*) as job_count 
                FROM job_postings 
                WHERE api_url != ''
                GROUP BY api_url 
                ORDER BY job_count DESC 
                LIMIT 10
            """)
            job_category_stats = cursor.fetchall()
            
            # ì§€ì—­ë³„ í†µê³„
            cursor.execute("""
                SELECT location, COUNT(*) as job_count 
                FROM job_postings 
                WHERE location != ''
                GROUP BY location 
                ORDER BY job_count DESC 
                LIMIT 10
            """)
            top_locations = cursor.fetchall()
            
            # ì—°ë´‰ ê´€ë ¨ í†µê³„
            cursor.execute("""
                SELECT COUNT(*) 
                FROM job_postings 
                WHERE salary != '' AND salary NOT LIKE '%í˜‘ì˜%'
            """)
            salary_disclosed = cursor.fetchone()[0]
            
            # ê²½ë ¥ë³„ í†µê³„
            cursor.execute("""
                SELECT career_level, COUNT(*) as count
                FROM job_postings 
                WHERE career_level != ''
                GROUP BY career_level 
                ORDER BY count DESC
            """)
            career_stats = cursor.fetchall()
            
            # ì¬íƒê·¼ë¬´ ê°€ëŠ¥ ê³µê³ 
            cursor.execute("""
                SELECT COUNT(*) 
                FROM job_postings 
                WHERE work_location_type LIKE '%ì¬íƒ%' OR work_location_type LIKE '%ì›ê²©%'
                   OR tags LIKE '%ì¬íƒ%' OR tags LIKE '%ì›ê²©%'
            """)
            remote_jobs = cursor.fetchone()[0]
            
            # ìµœê·¼ í¬ë¡¤ë§ ë¡œê·¸
            cursor.execute("""
                SELECT search_type, successfully_crawled, created_at
                FROM crawling_logs 
                ORDER BY created_at DESC 
                LIMIT 10
            """)
            recent_logs = cursor.fetchall()
            
            conn.close()
            
            return {
                "total_jobs": total_jobs,
                "top_companies": top_companies,
                "tech_stats": tech_stats,
                "job_category_stats": job_category_stats,
                "top_locations": top_locations,
                "salary_disclosed": salary_disclosed,
                "career_stats": career_stats,
                "remote_jobs": remote_jobs,
                "recent_logs": recent_logs
            }
            
        except Exception as e:
            logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def run_full_crawling(self, max_pages_per_search: int = 5):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸš€ ì í• ì „ì²´ í¬ë¡¤ë§ ì‹œì‘!")
        logger.info(f"ğŸ¯ ê²€ìƒ‰ ìœ í˜•: {len(self.search_params)}ê°œ")
        logger.info(f"ğŸ“„ ê²€ìƒ‰ë‹¹ ìµœëŒ€ í˜ì´ì§€: {max_pages_per_search}")
        logger.info("=" * 60)
        
        overall_start = datetime.now()
        all_jobs = []
        total_searches = len(self.search_params)
        
        for idx, (search_name, params) in enumerate(self.search_params.items(), 1):
            logger.info(f"ğŸ” [{idx}/{total_searches}] '{search_name}' ê²€ìƒ‰ ì‹œì‘...")
            
            search_start = datetime.now()
            
            try:
                # í•´ë‹¹ ê²€ìƒ‰ ì¡°ê±´ìœ¼ë¡œ í¬ë¡¤ë§
                search_jobs = self.crawl_search_type(search_name, params, max_pages_per_search)
                
                if search_jobs:
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    saved_count = self.save_to_database(search_jobs)
                    all_jobs.extend(search_jobs)
                    
                    search_end = datetime.now()
                    duration = (search_end - search_start).total_seconds()
                    
                    # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
                    self.save_crawling_log(
                        search_name, len(search_jobs), saved_count, 0,
                        search_start.strftime('%Y-%m-%d %H:%M:%S'),
                        search_end.strftime('%Y-%m-%d %H:%M:%S'),
                        duration
                    )
                    
                    logger.info(f"âœ… '{search_name}' ì™„ë£Œ: {len(search_jobs)}ê°œ ìˆ˜ì§‘, {saved_count}ê°œ ì €ì¥")
                else:
                    logger.warning(f"âš ï¸ '{search_name}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                
            except Exception as e:
                logger.error(f"âŒ '{search_name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê²€ìƒ‰ ê°„ íœ´ì‹ (ë§ˆì§€ë§‰ ê²€ìƒ‰ì´ ì•„ë‹Œ ê²½ìš°)
            if idx < total_searches:
                rest_time = random.uniform(10, 20)
                logger.info(f"ğŸ’¤ ë‹¤ìŒ ê²€ìƒ‰ ì „ {rest_time:.1f}ì´ˆ íœ´ì‹...")
                time.sleep(rest_time)
                logger.info("-" * 40)
        
        overall_end = datetime.now()
        total_duration = (overall_end - overall_start).total_seconds()
        
        # ìµœì¢… ê²°ê³¼
        logger.info("ğŸ‰ === ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ! ===")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_duration:.1f}ì´ˆ ({total_duration/60:.1f}ë¶„)")
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê³µê³ : {len(all_jobs)}ê°œ")
        
        # CSV ë‚´ë³´ë‚´ê¸°
        if all_jobs:
            csv_filename = self.export_to_csv()
            logger.info(f"ğŸ“„ CSV íŒŒì¼ ìƒì„±: {csv_filename}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        stats = self.get_database_stats()
        if stats:
            logger.info(f"ğŸ’¾ DB ì´ ê³µê³  ìˆ˜: {stats.get('total_jobs', 0)}ê°œ")
            
            logger.info("ğŸ¢ ìƒìœ„ ì±„ìš© íšŒì‚¬:")
            for company, count in stats.get('top_companies', [])[:5]:
                logger.info(f"   - {company}: {count}ê°œ")
            
            logger.info("ğŸ“ ì£¼ìš” ì§€ì—­:")
            for location, count in stats.get('top_locations', []):
                logger.info(f"   - {location}: {count}ê°œ")
            
            logger.info(f"ğŸ Python ê´€ë ¨: {stats.get('python_jobs', 0)}ê°œ")
            logger.info(f"â˜• Java ê´€ë ¨: {stats.get('java_jobs', 0)}ê°œ")
        
        logger.info("=" * 60)
        return len(all_jobs)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì í•(Jumpit) ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ v2.0")
    print("=" * 60)
    print("ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:")
    print("   - ì í• ì‹¤ì œ ì§ë¬´ ë¶„ë¥˜ ê¸°ë°˜ í¬ë¡¤ë§")
    print("   - API ê¸°ë°˜ ì•ˆì „í•œ í¬ë¡¤ë§")
    print("   - ì„œë²„ ë¶€í•˜ ìµœì†Œí™” (ëœë¤ ì§€ì—°)")
    print("   - SQLite DB ìë™ ì €ì¥")
    print("   - CSV íŒŒì¼ ë‚´ë³´ë‚´ê¸°")
    print("   - ìƒì„¸ ë¡œê¹… ì‹œìŠ¤í…œ")
    print("   - ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬")
    print("=" * 60)
    print("ğŸ¯ ì£¼ìš” ê²€ìƒ‰ ëŒ€ìƒ:")
    print("   ğŸ’» ê°œë°œ: ì„œë²„/ë°±ì—”ë“œ, í”„ë¡ íŠ¸ì—”ë“œ, í’€ìŠ¤íƒ, ëª¨ë°”ì¼")
    print("   ğŸ”§ ì¸í”„ë¼: DevOps, DBA, ì‹œìŠ¤í…œì—”ì§€ë‹ˆì–´")
    print("   ğŸ“Š ë°ì´í„°: ë¹…ë°ì´í„°, AI/ML ì—”ì§€ë‹ˆì–´")
    print("   ğŸ¨ ê¸°íƒ€: QA, PM, ë³´ì•ˆ, ë¸”ë¡ì²´ì¸")
    print("   ğŸ  íŠ¹ë³„: ì¬íƒê·¼ë¬´, ì‹ ì…í™˜ì˜, ê³ ì—°ë´‰")
    print("   ğŸ’» ê¸°ìˆ ìŠ¤íƒ: Java, Python, React, AWS ë“±")
    print("=" * 60)
    
    try:
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = JumpitCrawler()
        
        # ì‚¬ìš©ì ì„ íƒ
        print("\nì‹¤í–‰ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì „ì²´ í¬ë¡¤ë§ (ëª¨ë“  ê²€ìƒ‰ ì¡°ê±´)")
        print("2. íŠ¹ì • ê²€ìƒ‰ë§Œ ì‹¤í–‰")
        print("3. ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ë§Œ ë³´ê¸°")
        print("4. CSV ë‚´ë³´ë‚´ê¸°ë§Œ ì‹¤í–‰")
        
        choice = input("\nì„ íƒ (1-4): ").strip()
        
        if choice == "1":
            # ì „ì²´ í¬ë¡¤ë§
            pages_input = input("ê²€ìƒ‰ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’ 5): ").strip()
            max_pages = int(pages_input) if pages_input.isdigit() else 5
            
            print(f"\nğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (í˜ì´ì§€ë‹¹ ìµœëŒ€ {max_pages}ê°œ)...")
            total_jobs = crawler.run_full_crawling(max_pages)
            
            if total_jobs > 0:
                print(f"\nğŸ‰ í¬ë¡¤ë§ ì„±ê³µ! ì´ {total_jobs}ê°œ ì±„ìš©ê³µê³  ìˆ˜ì§‘")
                print("ğŸ“ íŒŒì¼ ìœ„ì¹˜:")
                print(f"   - ë°ì´í„°ë² ì´ìŠ¤: {crawler.db_name}")
                print(f"   - ë¡œê·¸ íŒŒì¼: jumpit_crawler.log")
                
                # ìë™ìœ¼ë¡œ CSVë„ ìƒì„±
                csv_file = crawler.export_to_csv()
                if csv_file:
                    print(f"   - CSV íŒŒì¼: {csv_file}")
            else:
                print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        elif choice == "2":
            # íŠ¹ì • ê²€ìƒ‰ ì„ íƒ
            print("\nê²€ìƒ‰ ì¡°ê±´ì„ ì„ íƒí•˜ì„¸ìš”:")
            search_list = list(crawler.search_params.keys())
            for i, search_name in enumerate(search_list, 1):
                print(f"{i}. {search_name}")
            
            search_choice = input(f"\nì„ íƒ (1-{len(search_list)}): ").strip()
            
            if search_choice.isdigit() and 1 <= int(search_choice) <= len(search_list):
                selected_search = search_list[int(search_choice) - 1]
                selected_params = crawler.search_params[selected_search]
                
                pages_input = input("ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’ 10): ").strip()
                max_pages = int(pages_input) if pages_input.isdigit() else 10
                
                print(f"\nğŸ¯ '{selected_search}' ê²€ìƒ‰ ì‹œì‘...")
                
                search_start = datetime.now()
                jobs = crawler.crawl_search_type(selected_search, selected_params, max_pages)
                
                if jobs:
                    saved_count = crawler.save_to_database(jobs)
                    search_end = datetime.now()
                    duration = (search_end - search_start).total_seconds()
                    
                    crawler.save_crawling_log(
                        selected_search, len(jobs), saved_count, 0,
                        search_start.strftime('%Y-%m-%d %H:%M:%S'),
                        search_end.strftime('%Y-%m-%d %H:%M:%S'),
                        duration
                    )
                    
                    print(f"âœ… ì™„ë£Œ! {len(jobs)}ê°œ ìˆ˜ì§‘, {saved_count}ê°œ ì €ì¥")
                    
                    # CSV ë‚´ë³´ë‚´ê¸° ì œì•ˆ
                    export_csv = input("CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    if export_csv == 'y':
                        csv_file = crawler.export_to_csv()
                        if csv_file:
                            print(f"ğŸ“„ CSV íŒŒì¼ ìƒì„±: {csv_file}")
                else:
                    print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        
        elif choice == "3":
            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
            print("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì¤‘...")
            stats = crawler.get_database_stats()
            
            if stats:
                print(f"\nğŸ’¾ ì „ì²´ ì±„ìš©ê³µê³ : {stats.get('total_jobs', 0)}ê°œ")
                
                print("\nğŸ¢ ìƒìœ„ ì±„ìš© íšŒì‚¬:")
                for company, count in stats.get('top_companies', [])[:7]:
                    print(f"   {company}: {count}ê°œ")
                
                print("\nğŸ“ ì§€ì—­ë³„ ë¶„í¬:")
                for location, count in stats.get('top_locations', [])[:7]:
                    print(f"   {location}: {count}ê°œ")
                
                print("\nğŸ’» ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒ:")
                tech_stats = stats.get('tech_stats', {})
                # ê¸°ìˆ ìŠ¤íƒì„ ì‚¬ìš© ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_techs = sorted(tech_stats.items(), key=lambda x: x[1], reverse=True)
                for tech, count in sorted_techs[:10]:  # ìƒìœ„ 10ê°œë§Œ
                    print(f"   {tech}: {count}ê°œ")
                
                print(f"\nğŸ  ì¬íƒê·¼ë¬´ ê°€ëŠ¥ ê³µê³ : {stats.get('remote_jobs', 0)}ê°œ")
                print(f"ğŸ’° ì—°ë´‰ ê³µê°œ ê³µê³ : {stats.get('salary_disclosed', 0)}ê°œ")
                
                print("\nğŸ‘¨â€ğŸ’¼ ê²½ë ¥ë³„ ë¶„í¬:")
                for career, count in stats.get('career_stats', [])[:5]:
                    print(f"   {career}: {count}ê°œ")
                
                print("\nğŸ¯ ì§ë¬´ë³„ ìˆ˜ì§‘ í˜„í™©:")
                for job_type, count in stats.get('job_category_stats', [])[:8]:
                    # API URLì—ì„œ ì˜ë¯¸ìˆëŠ” ì´ë¦„ ì¶”ì¶œ
                    display_name = job_type.replace('_', ' ').title() if job_type else 'ê¸°íƒ€'
                    print(f"   {display_name}: {count}ê°œ")
                
                print("\nğŸ“ ìµœê·¼ í¬ë¡¤ë§ ë¡œê·¸:")
                for search_type, crawled, created_at in stats.get('recent_logs', [])[:7]:
                    print(f"   {created_at}: {search_type} - {crawled}ê°œ")
            else:
                print("âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨")
        
        elif choice == "4":
            # CSV ë‚´ë³´ë‚´ê¸°ë§Œ
            print("\nğŸ“„ CSV ë‚´ë³´ë‚´ê¸° ì¤‘...")
            csv_file = crawler.export_to_csv()
            if csv_file:
                print(f"âœ… CSV íŒŒì¼ ìƒì„± ì™„ë£Œ: {csv_file}")
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                try:
                    file_size = os.path.getsize(csv_file)
                    print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size / 1024:.1f} KB")
                except:
                    pass
            else:
                print("âŒ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨")
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ë©”ì¸ í•¨ìˆ˜ ì˜¤ë¥˜: {e}")
    
    print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

def run_quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª ì í• API ì—°ê²° í…ŒìŠ¤íŠ¸...")
    
    crawler = JumpitCrawler()
    
    # ì¬íƒê·¼ë¬´ ê³µê³  1í˜ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸
    test_jobs = crawler.crawl_search_type("ì¬íƒê·¼ë¬´_í…ŒìŠ¤íŠ¸", {"tag": "WORK_AT_HOME_COMPANY"}, max_pages=1)
    
    if test_jobs:
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! {len(test_jobs)}ê°œ ê³µê³  ë°œê²¬")
        
        # ì²« ë²ˆì§¸ ê³µê³  ì •ë³´ ì¶œë ¥
        if test_jobs:
            job = test_jobs[0]
            print(f"\nğŸ“‹ ìƒ˜í”Œ ê³µê³ :")
            print(f"   ì œëª©: {job.title}")
            print(f"   íšŒì‚¬: {job.company_name}")
            print(f"   ìœ„ì¹˜: {job.location}")
            print(f"   ê¸°ìˆ ìŠ¤íƒ: {job.tech_stacks}")
            
        # ê°„ë‹¨ ì €ì¥ í…ŒìŠ¤íŠ¸
        saved = crawler.save_to_database(test_jobs[:3])  # ì²˜ìŒ 3ê°œë§Œ
        print(f"ğŸ’¾ DB ì €ì¥ í…ŒìŠ¤íŠ¸: {saved}ê°œ ì €ì¥ë¨")
        
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - API ì—°ê²° ë¬¸ì œ ê°€ëŠ¥ì„±")

if __name__ == "__main__":
    # ì‚¬ìš©ë²• ì˜ˆì‹œ
    print("ì‚¬ìš© ì˜µì…˜:")
    print("1. ì „ì²´ ì‹¤í–‰: python jumpit_crawler.py")
    print("2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python -c 'from jumpit_crawler import run_quick_test; run_quick_test()'")
    print()
    
    main()